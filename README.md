# candle-llm-chat

ä¸€ä¸ªåŸºäº [Candle](https://github.com/huggingface/candle) æœºå™¨å­¦ä¹ æ¡†æ¶çš„ LLM èŠå¤©æœºå™¨äººã€‚
æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨ã€æ”¯æŒæµå¼è¾“å‡ºå’Œ GPU åŠ é€Ÿçš„èŠå¤©æœºå™¨äººå®ç°ï¼Œæ”¯æŒå¤šç§ GGUF æ ¼å¼çš„é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ã€‚é‡‡ç”¨ç°ä»£åŒ–çš„ Rust å¼‚æ­¥è®¾è®¡ï¼Œå…·æœ‰ç®€æ´çš„æ¨¡å‹é…ç½®ç³»ç»Ÿã€‚

## âœ¨ åŠŸèƒ½ç‰¹æ€§

-   **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ Qwen2/Qwen3/Llama ç³»åˆ—æ¨¡å‹ï¼Œé€šè¿‡ `models.toml` é…ç½®æ–‡ä»¶ç®¡ç†
-   **ç®€æ´çš„ API**: åŸºäºå­—ç¬¦ä¸²æ ‡è¯†ç¬¦çš„æ¨¡å‹é€‰æ‹©ï¼Œæ”¯æŒ `"qwen3"` æˆ– `"qwen3.W3_14b"` æ ¼å¼
-   **æµå¼è¾“å‡º**: å®ç°æ‰“å­—æœºæ•ˆæœçš„å®æ—¶å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
-   **GPU åŠ é€Ÿ**: æ”¯æŒ CUDAï¼Œå¯åˆ©ç”¨ NVIDIA GPU è¿›è¡Œé«˜æ•ˆæ¨ç†
-   **å¼‚æ­¥å¤„ç†**: åŸºäº Tokio çš„å¼‚æ­¥è®¾è®¡ï¼Œç¡®ä¿åº”ç”¨æ€§èƒ½
-   **æ™ºèƒ½èŠå¤©ä¸Šä¸‹æ–‡**: è‡ªåŠ¨è§’è‰²åˆ‡æ¢å’Œæ€è€ƒè¿‡ç¨‹è¿‡æ»¤çš„ `ChatContext` ç®¡ç†
-   **é…ç½®çµæ´»**: é€šè¿‡ `InferenceConfig` ç»“æ„ä½“å’Œ TOML æ–‡ä»¶è½»æ¾è°ƒæ•´æ¨¡å‹å‚æ•°

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

-   Rust å·¥å…·é“¾ (æ¨èæœ€æ–°ç¨³å®šç‰ˆ)
-   CUDA å·¥å…·åŒ… (è‹¥éœ€ä½¿ç”¨ GPU åŠ é€Ÿ)

### 2. ä¸‹è½½ä¸è¿è¡Œ

```bash
git clone https://github.com/your-username/candle-llm-chat.git # æ›¿æ¢ä¸ºæ‚¨çš„ä»“åº“åœ°å€
cd candle-llm-chat
```

#### è®¾ç½®ä»£ç† (å¯é€‰)

å¦‚æœåœ¨ä¸­å›½å¤§é™†æˆ–å…¶ä»–ç½‘ç»œå—é™åœ°åŒºä¸‹è½½ Hugging Face æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦è®¾ç½®ä»£ç†ã€‚é¡¹ç›®æä¾›äº† `ProxyGuard` å·¥å…·ç±»ï¼š

```rust
use candle_llm_chat::utils::proxy::ProxyGuard;

// è®¾ç½®ä»£ç†ï¼ŒProxyGuard ä¼šåœ¨ä½œç”¨åŸŸç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†
let _proxy = ProxyGuard::new("7890"); // ç«¯å£å·ï¼Œå®Œæ•´åœ°å€ä¸º http://127.0.0.1:7890
```

`ProxyGuard` å®ç°äº† RAII æ¨¡å¼ï¼Œä¼šåœ¨ææ„æ—¶è‡ªåŠ¨æ¸…ç†ç¯å¢ƒå˜é‡ã€‚

### 3. è¿è¡Œæµ‹è¯•

**äº¤äº’å¼èŠå¤©æµ‹è¯•**ï¼š
```bash
cargo test --package candle-llm-chat --lib pipe::tests::test_pipeline -- --nocapture
```

**é¢„è®¾å¯¹è¯æµ‹è¯•**ï¼š
```bash
cargo test --package candle-llm-chat --lib pipe::tests::test_prompt -- --nocapture
```

è¿™äº›æµ‹è¯•å°†æ¼”ç¤ºæ¨¡å‹åŠ è½½ã€èŠå¤©ä¸Šä¸‹æ–‡ç®¡ç†å’Œæµå¼è¾“å‡ºåŠŸèƒ½ã€‚

### 4. æ¨¡å‹é…ç½®

é¡¹ç›®æ”¯æŒå¤šç§é¢„é…ç½®æ¨¡å‹ï¼Œåœ¨ `models.toml` ä¸­å®šä¹‰ï¼š

- **Qwen2 ç³»åˆ—**: 1.5B, 7B, 14B å‚æ•°æ¨¡å‹
- **Qwen3 ç³»åˆ—**: 4B, 8B, 14B, 32B å‚æ•°æ¨¡å‹  
- **Llama ç³»åˆ—**: åŒ…å« DeepSeek-R1-Distill-Llama-8B

é»˜è®¤ä½¿ç”¨ Qwen3-8B æ¨¡å‹ï¼Œå¯é€šè¿‡æ¨¡å‹æ ‡è¯†ç¬¦åˆ‡æ¢ï¼š

```rust
// ä½¿ç”¨é»˜è®¤æ¨¡å‹ (qwen3-8B)
let text_gen = TextGeneration::default().await?;

// ä½¿ç”¨æ¶æ„çš„é»˜è®¤æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen2").await?;

// ä½¿ç”¨ç‰¹å®šæ¨¡å‹å˜ä½“
let text_gen = TextGeneration::with_default_config("qwen3.W3_14b").await?;
```

## âš™ï¸ é…ç½®

### ä¸»è¦é…ç½®æ–‡ä»¶

**`src/model/config.rs`** - æ ¸å¿ƒé…ç½®ç»“æ„ï¼š

```rust
pub struct InferenceConfig {
    pub sample_len: usize,      // ç”Ÿæˆå“åº”çš„æœ€å¤§ token æ•°é‡ (é»˜è®¤: 1000)
    pub temperature: f64,       // æ§åˆ¶éšæœºæ€§ (é»˜è®¤: 0.8)
    pub top_p: Option<f64>,     // Nucleus é‡‡æ ·æ¦‚ç‡
    pub seed: u64,              // éšæœºç§å­ (é»˜è®¤: 299792458)
    pub repeat_penalty: f32,    // é‡å¤æƒ©ç½šç³»æ•° (é»˜è®¤: 1.1)
    pub repeat_last_n: usize,   // é‡å¤æƒ©ç½šä¸Šä¸‹æ–‡é•¿åº¦ (é»˜è®¤: 64)
    pub device: Device,         // è®¡ç®—è®¾å¤‡ (CPU/CUDA)
}
```

**`models.toml`** - æ¨¡å‹ä»“åº“é…ç½®ï¼š

```toml
[qwen3.W3_8b]
model_repo = "Qwen/Qwen3-8B-GGUF"
model_file = "Qwen3-8B-Q4_K_M"
tokenizer_repo = "Qwen/Qwen3-8B"
default = true
```

**`config.toml`** - HuggingFace è®¿é—®ä»¤ç‰Œç­‰å…¨å±€é…ç½®

### ä½¿ç”¨ç¤ºä¾‹

**åŸºæœ¬èŠå¤©æµå¼è¾“å‡º**ï¼š

```rust
use candle_llm_chat::pipe::TextGeneration;
use futures_util::{StreamExt, pin_mut};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // ä½¿ç”¨é»˜è®¤é…ç½® (Qwen3-8B)
    let mut text_gen = TextGeneration::default().await?;
    
    // æµå¼èŠå¤©
    let stream = text_gen.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±");
    pin_mut!(stream);
    
    while let Some(Ok(token)) = stream.next().await {
        print!("{}", token);
    }
    
    Ok(())
}
```

**é€‰æ‹©ç‰¹å®šæ¨¡å‹**ï¼š

```rust
use candle_llm_chat::pipe::TextGeneration;

// ä½¿ç”¨ Qwen2 é»˜è®¤æ¨¡å‹ (7B)
let text_gen = TextGeneration::with_default_config("qwen2").await?;

// ä½¿ç”¨ Qwen3-14B æ¨¡å‹  
let text_gen = TextGeneration::with_default_config("qwen3.W3_14b").await?;

// ä½¿ç”¨ DeepSeek-R1-Llama-8B æ¨¡å‹
let text_gen = TextGeneration::with_default_config("llama.DeepseekR1Llama8b").await?;
```

**è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°**ï¼š

```rust
use candle_llm_chat::model::config::InferenceConfig;
use candle_llm_chat::pipe::TextGeneration;

let mut config = InferenceConfig::default();
config.temperature = 0.7;
config.sample_len = 2000;
config.repeat_penalty = 1.2;

let mut text_gen = TextGeneration::new("qwen3", config).await?;
```

## ğŸ“¦ GGUF æ¨¡å‹ä¸åˆ†ç‰‡å¤„ç†

æœ¬é¡¹ç›®æ”¯æŒ GGUF æ ¼å¼çš„æ¨¡å‹ã€‚å¯¹äºåˆ†ç‰‡çš„ GGUF æ¨¡å‹æ–‡ä»¶ï¼Œéœ€è¦ä½¿ç”¨ `llama-gguf-split` å·¥å…·è¿›è¡Œåˆå¹¶ã€‚

### ä¾èµ–: `llama-gguf-split`

`llama-gguf-split` æ˜¯ä¸€ä¸ªå¤–éƒ¨è¿è¡Œæ—¶ä¾èµ–ã€‚å¦‚æœéœ€è¦åŠ è½½åˆ†ç‰‡æ¨¡å‹ï¼Œè¯·ç¡®ä¿å·²æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…å¹¶å°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿ PATHï¼š

1.  å…‹éš† `llama.cpp` ä»“åº“:
    ```bash
    git clone --recursive https://github.com/ggerganov/llama.cpp
    ```
2.  ç¼–è¯‘å®‰è£…:
    ```bash
    cd llama.cpp
    cmake -S . -B build
    cmake --build build --config Release
    ```
3.  å°†ç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶ (é€šå¸¸åœ¨ `build/bin` ç›®å½•ä¸‹) æ·»åŠ åˆ°ç³»ç»Ÿ PATHã€‚

### è‡ªåŠ¨åˆå¹¶

ç¨‹åºåœ¨ä¸‹è½½æ¨¡å‹æ—¶ï¼Œå¦‚æœæ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶æ˜¯åˆ†ç‰‡çš„ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ `llama-gguf-split` è¿›è¡Œåˆå¹¶ã€‚åˆå¹¶åçš„å®Œæ•´æ¨¡å‹æ–‡ä»¶å°†ä¿å­˜åœ¨ä¸åˆ†ç‰‡æ–‡ä»¶ç›¸åŒçš„ç›®å½•ä¸‹ã€‚

å‚è€ƒèµ„æ–™:
- [How to use the gguf-split / Model sharding demo](https://github.com/ggml-org/llama.cpp/discussions/6404)

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·äº¤äº’å±‚"
        A[ç”¨æˆ·è¾“å…¥] --> B[TextGeneration::chat]
    end
    
    subgraph "é…ç½®ç®¡ç†å±‚"
        MR[ModelRegistry<br/>æ¨¡å‹æ³¨å†Œè¡¨] --> HI[HubInfo<br/>æ¨¡å‹ä»“åº“ä¿¡æ¯]
        MT[models.toml] --> MR
        HI --> ML[ModelLoader<br/>æ¨¡å‹åŠ è½½å™¨]
    end
    
    subgraph "æ ¸å¿ƒç»„ä»¶"
        C[ChatContext<br/>èŠå¤©ä¸Šä¸‹æ–‡ç®¡ç†] --> TG[TextGeneration<br/>æ–‡æœ¬ç”Ÿæˆç®¡é“]
        ML --> TG
        IC[InferenceConfig<br/>æ¨ç†é…ç½®] --> TG
        F[TokenOutputStream<br/>Tokenæµå¤„ç†] --> TG
        G[LogitsProcessor<br/>é‡‡æ ·å¤„ç†] --> TG
    end
    
    subgraph "æ¨¡å‹æŠ½è±¡å±‚"
        FW[Forward Trait<br/>ç»Ÿä¸€æ¨ç†æ¥å£] --> MW[ModelWeightså®ç°]
        MW --> MW1[quantized_qwen2::ModelWeights]
        MW --> MW2[quantized_qwen3::ModelWeights]
        MW --> MW3[quantized_llama::ModelWeights]
    end
    
    subgraph "æ¨¡å‹å®ç°å±‚"
        MW1 --> H1[Qwen2 GGUFæ¨¡å‹æ–‡ä»¶]
        MW2 --> H2[Qwen3 GGUFæ¨¡å‹æ–‡ä»¶]
        MW3 --> H3[Llama GGUFæ¨¡å‹æ–‡ä»¶]
        I[Tokenizer<br/>åˆ†è¯å™¨] --> TG
    end
    
    subgraph "åº•å±‚æ¡†æ¶"
        K[Candle Framework<br/>æœºå™¨å­¦ä¹ æ¡†æ¶]
        L[CUDA Support<br/>GPUåŠ é€Ÿ]
        M[HuggingFace Hub<br/>æ¨¡å‹ä»“åº“]
    end
    
    subgraph "å·¥å…·ç»„ä»¶"
        N[ProxyGuard<br/>ä»£ç†è®¾ç½®] --> M
        O[llama-gguf-split<br/>æ¨¡å‹åˆ†ç‰‡åˆå¹¶] --> H1
        O --> H2
        O --> H3
    end
    
    B --> C
    TG --> P[Stream Output<br/>æµå¼è¾“å‡º]
    P --> Q[å®æ—¶å“åº”æ˜¾ç¤º]
    
    H1 --> M
    H2 --> M
    H3 --> M
    I --> M
    MW1 --> K
    MW2 --> K
    MW3 --> K
    K --> L
    
    style MR fill:#fff3e0
    style HI fill:#e3f2fd
    style FW fill:#f1f8e9
    style C fill:#e1f5fe
    style TG fill:#f3e5f5
    style P fill:#e8f5e8
```

### æ¶æ„è¯´æ˜

#### æ ¸å¿ƒè®¾è®¡æ¨¡å¼

**1. ç®€åŒ–çš„é…ç½®ç³»ç»Ÿ**
- `ModelRegistry`: ä» `models.toml` åŠ è½½æ¨¡å‹é…ç½®çš„æ³¨å†Œè¡¨ç³»ç»Ÿ
- `HubInfo`: åŒ…å«æ¨¡å‹ä»“åº“ã€æ–‡ä»¶åå’Œåˆ†è¯å™¨ä»“åº“çš„é…ç½®ç»“æ„
- `InferenceConfig`: æ¨ç†å‚æ•°é…ç½®ï¼ŒåŒ…å«æ¸©åº¦ã€é‡‡æ ·é•¿åº¦ç­‰
- `ModelLoader`: ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½å™¨ï¼Œè´Ÿè´£åŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨å’Œå…ƒæ•°æ®

**2. æ¨¡å‹æ ‡è¯†ç¬¦ç³»ç»Ÿ**
```rust
// æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
// 1. æ¶æ„å - ä½¿ç”¨è¯¥æ¶æ„çš„é»˜è®¤æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3").await?;

// 2. æ¶æ„å.å˜ä½“å - ä½¿ç”¨ç‰¹å®šæ¨¡å‹å˜ä½“
let text_gen = TextGeneration::with_default_config("qwen3.W3_14b").await?;
```

**3. ç»Ÿä¸€æ¨ç†æ¥å£**
```rust
pub trait Forward {
    fn forward(&mut self, x: &Tensor, index_pos: usize) -> Result<Tensor>;
}

// é€šè¿‡å®ä¸ºæ‰€æœ‰æ¨¡å‹æƒé‡å®ç° Forward trait
impl_model_traits!(quantized_llama, quantized_qwen2, quantized_qwen3);
```

#### æ ¸å¿ƒæµç¨‹
1. **é…ç½®åŠ è½½** â†’ `ModelRegistry` ä» `models.toml` è¯»å–æ¨¡å‹é…ç½®
2. **æ¨¡å‹é€‰æ‹©** â†’ é€šè¿‡å­—ç¬¦ä¸²æ ‡è¯†ç¬¦ (å¦‚ `"qwen3"` æˆ– `"qwen3.W3_14b"`) é€‰æ‹©æ¨¡å‹
3. **å¼‚æ­¥åŠ è½½** â†’ `ModelLoader::load()` å¼‚æ­¥åŠ è½½ GGUF æ¨¡å‹ã€åˆ†è¯å™¨å’Œå…ƒæ•°æ®
4. **æ¨ç†æ‰§è¡Œ** â†’ `Forward` trait ç»Ÿä¸€æ¨ç†æ¥å£
5. **æµå¼è¾“å‡º** â†’ `TextGeneration::chat()` è¿”å›å¼‚æ­¥æµ

#### å…³é”®ç»„ä»¶
- **ModelRegistry**: TOML é…ç½®æ–‡ä»¶é©±åŠ¨çš„æ¨¡å‹æ³¨å†Œè¡¨ï¼Œæ”¯æŒé»˜è®¤æ¨¡å‹å’Œå˜ä½“é€‰æ‹©
- **HubInfo**: å°è£… HuggingFace æ¨¡å‹ä»“åº“ä¿¡æ¯ï¼Œè´Ÿè´£ä¸‹è½½æ¨¡å‹å’Œåˆ†è¯å™¨
- **ModelLoader**: ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½å™¨ï¼Œè¿”å› `(Box<dyn Forward>, Tokenizer, ModelInfo)` å…ƒç»„
- **ModelInfo**: ä» GGUF æ–‡ä»¶å…ƒæ•°æ®æå–çš„æ¨¡å‹ä¿¡æ¯ (æ¶æ„ã€EOS tokenã€èŠå¤©æ¨¡æ¿)
- **InferenceConfig**: æ¨ç†å‚æ•°é…ç½®ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¸©åº¦ã€é‡‡æ ·é•¿åº¦ç­‰
- **ChatContext**: æ™ºèƒ½èŠå¤©ä¸Šä¸‹æ–‡ï¼Œè‡ªåŠ¨è§’è‰²åˆ‡æ¢å’Œæ€è€ƒè¿‡ç¨‹è¿‡æ»¤
- **TextGeneration**: æ ¸å¿ƒæ–‡æœ¬ç”Ÿæˆç®¡é“ï¼Œæ”¯æŒæµå¼è¾“å‡º
- **å®ç³»ç»Ÿ**: `impl_model_traits!` è‡ªåŠ¨ä¸ºæ¨¡å‹å®ç°å¿…è¦ trait

#### æŠ€æœ¯ç‰¹æ€§
- ğŸ¯ **ç®€æ´ API**: åŸºäºå­—ç¬¦ä¸²çš„æ¨¡å‹é€‰æ‹©ï¼Œæ— éœ€å¤æ‚çš„æšä¸¾ç±»å‹
- ğŸ”§ **é…ç½®é©±åŠ¨**: é€šè¿‡ TOML æ–‡ä»¶ç®¡ç†æ¨¡å‹ï¼Œæ˜“äºæ‰©å±•æ–°æ¨¡å‹
- ğŸ”„ **å¼‚æ­¥ä¼˜å…ˆ**: å…¨å¼‚æ­¥è®¾è®¡ï¼Œæ¨¡å‹åŠ è½½å’Œæ¨ç†å‡ä¸ºå¼‚æ­¥
- ğŸš€ **GPU åŠ é€Ÿ**: è‡ªåŠ¨æ£€æµ‹ CUDA è®¾å¤‡ï¼Œæå‡æ¨ç†æ€§èƒ½
- ğŸ“¡ **æµå¼è¾“å‡º**: åŸºäº async-stream çš„å®æ—¶å“åº”
- ğŸ› ï¸ **ç®€åŒ–æ¶æ„**: ç§»é™¤å¤æ‚çš„æ³›å‹ç³»ç»Ÿï¼Œé‡‡ç”¨æ›´ç›´è§‚çš„å­—ç¬¦ä¸²æ ‡è¯†ç¬¦

## ğŸ“ é¡¹ç›®ç»“æ„

```
src/
â”œâ”€â”€ lib.rs                 # åº“å…¥å£
â”œâ”€â”€ pipe.rs                # TextGeneration æ ¸å¿ƒç®¡é“
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ mod.rs            # Forward trait å’Œå®å®šä¹‰
â”‚   â”œâ”€â”€ config.rs         # InferenceConfig å’Œ ModelLoader
â”‚   â”œâ”€â”€ registry.rs       # ModelRegistry æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â””â”€â”€ hub.rs            # HubInfo å’Œ ModelInfo
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ mod.rs            # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ load.rs           # æ¨¡å‹å’Œåˆ†è¯å™¨ä¸‹è½½
â”‚   â”œâ”€â”€ chat.rs           # ChatContext èŠå¤©ä¸Šä¸‹æ–‡
â”‚   â””â”€â”€ proxy.rs          # ProxyGuard ä»£ç†ç®¡ç†

é…ç½®æ–‡ä»¶:
â”œâ”€â”€ models.toml           # æ¨¡å‹ä»“åº“é…ç½®
â”œâ”€â”€ config.toml           # å…¨å±€é…ç½® (HF token ç­‰)
â””â”€â”€ Cargo.toml            # é¡¹ç›®ä¾èµ–
```

## ğŸ”§ æ‰©å±•æ–°æ¨¡å‹

æ·»åŠ æ–°æ¨¡å‹åªéœ€è¦ä¸¤æ­¥ï¼š

1. **åœ¨ `models.toml` ä¸­æ·»åŠ é…ç½®**ï¼š
```toml
[qwen3.W3_72b]
model_repo = "Qwen/Qwen3-72B-GGUF"
model_file = "Qwen3-72B-Q4_K_M"
tokenizer_repo = "Qwen/Qwen3-72B"
```

2. **åœ¨ä»£ç ä¸­ä½¿ç”¨**ï¼š
```rust
let text_gen = TextGeneration::with_default_config("qwen3.W3_72b").await?;
```

å¯¹äºæ–°çš„æ¨¡å‹æ¶æ„ï¼Œéœ€è¦ï¼š
- åœ¨ `models.toml` ä¸­æ·»åŠ æ–°çš„æ¶æ„éƒ¨åˆ† (å¦‚ `[new_arch.variant]`)
- åœ¨ `ModelLoader::load()` ä¸­æ·»åŠ å¯¹åº”çš„åŠ è½½é€»è¾‘
- ç¡®ä¿ Candle æ¡†æ¶æ”¯æŒè¯¥æ¨¡å‹æ¶æ„

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚
