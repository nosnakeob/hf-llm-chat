[package]
name = "hf-llm-chat"
version = "0.3.1"
edition = "2024"

[lints.rust]
unused = "allow"

[dependencies]
anyhow = "1.0"
tokio = "1.45"
# intel-mkl-src = { version = "0.8", features = ["mkl-static-lp64-iomp"] }

candle = { package = "candle-core", git = "https://github.com/huggingface/candle.git" }
candle-nn = { git = "https://github.com/huggingface/candle.git" }
# feat mkl STATUS_DLL_NOT_FOUND
candle-transformers = { git = "https://github.com/huggingface/candle.git", features = [
    "cuda",
    "cudnn",
    "flash-attn",
] }
candle-examples = { git = "https://github.com/huggingface/candle.git" }

hf-hub = { version = "0.4", features = ["tokio"] }
tokenizers = "0.21"

async-stream = "0.3"
futures-core = "0.3"
futures-util = "0.3"

tracing = "0.1"

which = "7.0"

config = "0.15"
serde_json = "1.0"
serde = { version = "1.0", features = ["derive"] }
derive-new = "0.7"
minijinja = { version = "2.12", features = ["loader"] }
minijinja-contrib = { version = "2.12", features = ["pycompat"] }

[dev-dependencies]
tracing-subscriber = "0.3"
